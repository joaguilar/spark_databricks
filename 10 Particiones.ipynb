{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0514b603-1c3b-46b7-a94e-29718bb57e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType, TimestampType, LongType\n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType, DecimalType\n",
    "from pyspark.sql.functions import regexp_extract, split, from_unixtime, col, avg, min, max, desc\n",
    "from pyspark.sql.functions import grouping, explode, array_contains, struct, collect_list, row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9ac482-0cf3-4d67-b654-3be167c477f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/PADRON_COMPLETO.csv</td><td>PADRON_COMPLETO.csv</td><td>437542254</td><td>1750034299000</td></tr><tr><td>dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/distelec.csv</td><td>distelec.csv</td><td>175692</td><td>1750034273000</td></tr><tr><td>dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/movies.csv</td><td>movies.csv</td><td>4192335</td><td>1750034274000</td></tr><tr><td>dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/padron_limpio.csv/</td><td>padron_limpio.csv/</td><td>0</td><td>1750039677659</td></tr><tr><td>dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/por-ciclo-2016-2018.csv</td><td>por-ciclo-2016-2018.csv</td><td>904931</td><td>1750034273000</td></tr><tr><td>dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/ratings_full.csv</td><td>ratings_full.csv</td><td>933898879</td><td>1750034299000</td></tr><tr><td>dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/tags.csv</td><td>tags.csv</td><td>85361813</td><td>1750034296000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/PADRON_COMPLETO.csv",
         "PADRON_COMPLETO.csv",
         437542254,
         1750034299000
        ],
        [
         "dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/distelec.csv",
         "distelec.csv",
         175692,
         1750034273000
        ],
        [
         "dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/movies.csv",
         "movies.csv",
         4192335,
         1750034274000
        ],
        [
         "dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/padron_limpio.csv/",
         "padron_limpio.csv/",
         0,
         1750039677659
        ],
        [
         "dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/por-ciclo-2016-2018.csv",
         "por-ciclo-2016-2018.csv",
         904931,
         1750034273000
        ],
        [
         "dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/ratings_full.csv",
         "ratings_full.csv",
         933898879,
         1750034299000
        ],
        [
         "dbfs:/Volumes/big_data_ii_2025/spark_examples/spark_data/tags.csv",
         "tags.csv",
         85361813,
         1750034296000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /Volumes/big_data_ii_2025/spark_examples/spark_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f54704a2-6255-4a53-8e7b-6a34e0c200d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Manejo y Optimización de Particiones en Spark\n",
    "\n",
    "En este notebook aprenderás a:\n",
    "- Visualizar y modificar el número de particiones de un DataFrame.\n",
    "- Usar las funciones `repartition` y `coalesce`.\n",
    "- Guardar datos con particionado por columna y medir el impacto en el rendimiento.\n",
    "Trabajaremos con el dataset MovieLens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d116c3e8-f24a-44ce-a81a-6d8beaee14e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cargar datos MovieLens (ratings.csv) suponiendo que está en /dbfs/FileStore/movielens/ratings.csv\n",
    "# Tabla Ratings\n",
    "ratings_schema  = StructType(fields=[\n",
    "    StructField(\"userId\",IntegerType(),True), \n",
    "    StructField(\"movieId\",IntegerType(),True),\n",
    "    StructField(\"rating\",DecimalType(precision=2,scale=1),True),\n",
    "    StructField(\"timestamp\",LongType(),True)\n",
    "])\n",
    "ratingsDf = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"dateFormat\", \"yyyyMMdd\")\\\n",
    "    .schema(ratings_schema)\\\n",
    "    .csv(\"/Volumes/big_data_ii_2025/spark_examples/spark_data/ratings_full.csv\")\\\n",
    "    .withColumn(\\\n",
    "            \"date\",\\\n",
    "            from_unixtime(\"timestamp\", \"yyyyMMdd\"))\\\n",
    "                .drop('timestamp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79827207-d504-4460-aa5e-fe6207df5b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: decimal(2,1) (nullable = true)\n |-- date: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "ratingsDf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4397f78-d70c-4ed8-883c-d8779e86128b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings count: 33,832,162\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ratings count: {ratingsDf.count():,.0f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c603d5b-fb4e-4587-8f81-1e136c3a698e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkNotImplementedError\u001B[0m                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6212165207225491>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Ver número de particiones\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParticiones iniciales:\u001B[39m\u001B[38;5;124m\"\u001B[39m, ratingsDf\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mgetNumPartitions())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2432\u001B[0m, in \u001B[0;36mDataFrame.rdd\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   2430\u001B[0m \u001B[38;5;129m@property\u001B[39m\n",
       "\u001B[1;32m   2431\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrdd\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Row]\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m-> 2432\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n",
       "\u001B[1;32m   2433\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2434\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n",
       "\u001B[1;32m   2435\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mPySparkNotImplementedError\u001B[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkNotImplementedError",
        "evalue": "[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "metadata": {
        "errorSummary": "[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "NOT_IMPLEMENTED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkNotImplementedError\u001B[0m                Traceback (most recent call last)",
        "File \u001B[0;32m<command-6212165207225491>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Ver número de particiones\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParticiones iniciales:\u001B[39m\u001B[38;5;124m\"\u001B[39m, ratingsDf\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mgetNumPartitions())\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2432\u001B[0m, in \u001B[0;36mDataFrame.rdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2430\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   2431\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrdd\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Row]\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 2432\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n\u001B[1;32m   2433\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2434\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m   2435\u001B[0m     )\n",
        "\u001B[0;31mPySparkNotImplementedError\u001B[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ver número de particiones\n",
    "print(\"Particiones iniciales:\", ratingsDf.rdd.getNumPartitions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5931bea1-f3b5-4eb1-9b89-f2d72cf0ddd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de repartition(16): 16\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Cambiar el número de particiones a 16\n",
    "df_repart = ratingsDf.repartition(16)\n",
    "print(\"Después de repartition(16):\", df_repart.rdd.getNumPartitions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ca0dd3-0974-4e9d-8247-035c1e869e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de coalesce(2): 2\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Reducir a 2 particiones con coalesce\n",
    "df_coalesce = df_repart.coalesce(2)\n",
    "print(\"Después de coalesce(2):\", df_coalesce.rdd.getNumPartitions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5541cc00-8afe-4db9-a0e1-f50809334a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\nWall time: 6.91 µs\nTiempo: 104.14708518981934 segundos\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Guardar datos particionando por 'rating'\n",
    "import time\n",
    "start = time.time()\n",
    "output_path = \"/Volumes/big_data_ii_2025/spark_examples/spark_data/rating_by_rating\"\n",
    "ratingsDf.write.mode(\"overwrite\").partitionBy(\"rating\").parquet(output_path)\n",
    "print(\"Tiempo:\", time.time() - start, \"segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716b8cc9-08e5-4b17-9aab-5a12e832cfd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n4 drwxr-xr-x 2 root root 4096 Jun  8 20:18 .\n4 drwxrwxrwt 1 root root 4096 Jun  8 20:18 ..\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls -las /Volumes/big_data_ii_2025/spark_examples/spark_data/rating_by_rating/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff01e306-715f-4ade-980e-8a506150eb49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particiones originales: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Particiones originales:\", ratingsDf.rdd.getNumPartitions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5dd4b5a-f3c9-44d2-b28e-c4a5d18b237b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def medir_tiempo_particiones(df, num_particiones):\n",
    "    df_mod = df.repartition(num_particiones)\n",
    "    print(f\"\\nUsando {num_particiones} particiones:\")\n",
    "    start = time.time()\n",
    "    # Operación costosa: groupBy + count\n",
    "    resultado = df_mod.groupBy(\"movieId\").count().collect()\n",
    "    print(\"Tiempo:\", round(time.time() - start, 2), \"segundos\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b085823-9aa3-4a3a-803a-25201121ff61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUsando 2 particiones:\nTiempo: 63.93 segundos\n\nUsando 4 particiones:\nTiempo: 63.58 segundos\n\nUsando 8 particiones:\nTiempo: 64.058 segundos\n\nUsando 16 particiones:\nTiempo: 65.2 segundos\n\nUsando 32 particiones:\nTiempo: 66.018 segundos\n"
     ]
    }
   ],
   "source": [
    "for n in [2, 4, 8, 16, 32]:\n",
    "    medir_tiempo_particiones(ratingsDf, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993327fd-43aa-440a-8cec-5b5b2792049d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6212165207225486,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "10 Particiones",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}