{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88073add",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b327071",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importamos y configuramos la sesión de Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Contribuciones2016-2018\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Mostrar configuración básica\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37f4a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
    "\n",
    "# Esquema para PADRON_COMPLETO.csv\n",
    "schema_padron = StructType([\n",
    "    StructField(\"CEDULA\", StringType(), nullable=False),\n",
    "    StructField(\"CODELEC\", StringType(), nullable=False),\n",
    "    StructField(\"RELLENO\", StringType(), nullable=True),\n",
    "    StructField(\"FECHACADUC\", StringType(), nullable=True),\n",
    "    StructField(\"JUNTA\", StringType(), nullable=True),\n",
    "    StructField(\"NOMBRE\", StringType(), nullable=True),\n",
    "    StructField(\"1_APELLIDO\", StringType(), nullable=True),\n",
    "    StructField(\"2_APELLIDO\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Esquema para distelec.csv\n",
    "schema_distr = StructType([\n",
    "    StructField(\"CODELE\", StringType(), nullable=False),\n",
    "    StructField(\"PROVINCIA\", StringType(), nullable=True),\n",
    "    StructField(\"CANTON\", StringType(), nullable=True),\n",
    "    StructField(\"DISTRITO\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Leemos PADRON_COMPLETO\n",
    "padron_df = spark.read.csv(\"/FileStore/tables/PADRON_COMPLETO.csv\",\n",
    "                           sep=\",\", header=True,\n",
    "                           schema=schema_padron,\n",
    "                           dateFormat=\"yyyyMMdd\")\n",
    "\n",
    "# Leemos distelec\n",
    "distr_df = spark.read.csv(\"/FileStore/tables/distelec.csv\",\n",
    "                          sep=\",\", header=True,\n",
    "                          schema=schema_distr)\n",
    "\n",
    "# Leemos contribuciones, sin encabezado útil, saltamos primeras 3 líneas\n",
    "raw_contrib = spark.read.option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", False) \\\n",
    "    .csv(\"/FileStore/tables/por-ciclo-2016-2018.csv\", sep=\",\", header=False)\n",
    "\n",
    "# Quitamos líneas de título manualmente\n",
    "contrib = raw_contrib.filter(raw_contrib._c0.isNull() == False) \\\n",
    "    .filter(raw_contrib._c0 != \"TRIBUNAL SUPREMO DE ELECCIONES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac0ae6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, regexp_replace, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Renombramos columnas relevantes y seleccionamos las necesarias\n",
    "contrib2 = contrib.select(\n",
    "    col(\"_c1\").alias(\"TIPO_CONTRIBUCION\"),\n",
    "    col(\"_c2\").alias(\"TIPO_PERSONA\"),\n",
    "    col(\"_c3\").alias(\"ESCALA\"),\n",
    "    col(\"_c4\").alias(\"PARTIDO\"),\n",
    "    col(\"_c5\").alias(\"FECHA_STR\"),\n",
    "    col(\"_c6\").alias(\"CEDULA\"),\n",
    "    col(\"_c7\").alias(\"NOMBRE_CONTRIBUYENTE\"),\n",
    "    col(\"_c8\").alias(\"MONTO_STR\")\n",
    ")\n",
    "\n",
    "# Convertimos FECHA a DateType (formato MM/dd/yy)\n",
    "contrib3 = contrib2.withColumn(\"FECHA\",\n",
    "    to_date(col(\"FECHA_STR\"), \"MM/dd/yy\")) \\\n",
    "  .drop(\"FECHA_STR\")\n",
    "\n",
    "# UDF para limpiar monto y convertir a Double\n",
    "@udf(returnType=DoubleType())\n",
    "def parse_monto(s):\n",
    "    if s is None: return None\n",
    "    return float(s.replace('\"','').replace(',',''))\n",
    "\n",
    "contrib4 = contrib3.withColumn(\"MONTO\",\n",
    "    parse_monto(col(\"MONTO_STR\"))) \\\n",
    "  .drop(\"MONTO_STR\")\n",
    "\n",
    "# Cacheamos para reutilizar\n",
    "contrib4.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7811a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Primero, mapeamos el padron para extraer CODELEC de 6 dígitos\n",
    "padron2 = padron_df.withColumn(\"CODELE\", col(\"CODELEC\"))\n",
    "\n",
    "# Join padron ↔ distritos para tener PROVINCIA/CANTON/DISTRITO en las contribuciones\n",
    "contrib_geo = contrib4.join(\n",
    "    padron2.select(\"CEDULA\",\"CODELE\"),\n",
    "    on=\"CEDULA\", how=\"left\"\n",
    ").join(\n",
    "    distr_df, on=\"CODELE\", how=\"left\"\n",
    ").cache()\n",
    "\n",
    "# Vemos esquema final\n",
    "contrib_geo.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aee5cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Contamos CEDULA distinta por partido\n",
    "cont_por_partido = contrib_geo.groupBy(\"PARTIDO\") \\\n",
    "    .agg(countDistinct(\"CEDULA\").alias(\"NUM_CONTRIBUYENTES\")) \\\n",
    "    .orderBy(col(\"NUM_CONTRIBUYENTES\").desc())\n",
    "\n",
    "cont_por_partido.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e03c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Filtramos provincia San José y agregamos suma de MONTO\n",
    "dist_sanjose = contrib_geo.filter(col(\"PROVINCIA\") == \"SAN JOSE\") \\\n",
    "    .groupBy(\"CANTON\") \\\n",
    "    .agg(\n",
    "      countDistinct(\"CEDULA\").alias(\"NUM_CONTRIBUYENTES\"),\n",
    "      # solo efectivo y especie juntas\n",
    "      expr(\"sum(MONTO) as MONTO_TOTAL\")\n",
    "    ).orderBy(col(\"MONTO_TOTAL\").desc())\n",
    "\n",
    "dist_sanjose.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ec2a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Separamos por tipo de contribución\n",
    "pivot_tipo = contrib_geo.groupBy(\"CANTON\") \\\n",
    "    .pivot(\"TIPO_CONTRIBUCION\", [\"EFECTIVO\",\"ESPECIE\"]) \\\n",
    "    .count().na.fill(0) \\\n",
    "    .orderBy(col(\"EFECTIVO\").desc())\n",
    "\n",
    "pivot_tipo.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e808d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "**Particiones y cache**  \n",
    "- `contrib_geo` está cached para acelerar múltiples lecturas.  \n",
    "- Podemos particionar por `CANTON` si vamos a escribir resultados a disco:\n",
    "\n",
    "```python\n",
    "contrib_geo.repartition(\"CANTON\") \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .parquet(\"/mnt/data/output/contrib_by_canton\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
