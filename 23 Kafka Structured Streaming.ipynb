{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a15a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.kafka:kafka-clients:3.5.1 pyspark-shell\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a6d45",
   "metadata": {},
   "source": [
    "Este bloque configura e inicializa Spark y Spark Streaming para trabajar con datos de Kafka.  \n",
    "\n",
    "Incluye la importación de módulos necesarios, la inicialización de SparkContext y StreamingContext, y la creación de una SparkSession con los paquetes requeridos para la integración con Kafka.  \n",
    "\n",
    "Esto permite procesar flujos de datos en tiempo real provenientes de Kafka usando PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/joseaguilar/Documents/Development/spark/spark-3.5.1-bin-hadoop3')\n",
    "from pyspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType, TimestampType, LongType\n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType, DecimalType\n",
    "from pyspark.sql.functions import regexp_extract, split, from_unixtime, col, avg, min, max\n",
    "from pyspark.sql.functions import grouping_id, window, explode, to_json, from_json\n",
    "from pyspark.sql.functions import udf, lit, current_timestamp, current_date, date_format\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "#from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Crear el contexto de Spark\n",
    "sc = None\n",
    "try:\n",
    "    sc = SparkContext(appName=\"kafkademo\")\n",
    "except:\n",
    "    sc = SparkContext.getOrCreate(\"kafkademo\")\n",
    "ssc = StreamingContext(sc, 10)  # Intervalo de 10 segundos\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"kafkademo\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \",\".join([\n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\",\n",
    "                \"org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.1\",\n",
    "                \"org.apache.kafka:kafka-clients:3.5.1\"\n",
    "            ])) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5393d1a",
   "metadata": {},
   "source": [
    "# Structured Streaming\n",
    "\n",
    "Para conectarse con Kafka, se utilia un paradigma diferente llamado **Structured Streaming**.\n",
    "\n",
    "Documentación: [Spark Streaming + Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "\n",
    "Mas general, documentación sobre Structured Streaming: [Spark Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html) sobre todo la sección de [Basic Concepts](https://spark.apache.org/docs/latest/streaming-programming-guide.html#basic-concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ceeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los JARs cargados en el contexto de Spark\n",
    "# Validar que esté cargado el paquete de Kafka\n",
    "print(\"JARs cargados en el contexto de Spark:\")\n",
    "print(\"========================================\")\n",
    "java_jars = spark.sparkContext._jsc.sc().listJars()\n",
    "jars = [java_jars.apply(i) for i in range(java_jars.length())]\n",
    "for jar in jars:\n",
    "    print(jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la dirección del servidor de Kafka\n",
    "kafka_server = \"http://localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b011b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer datos en streaming desde el tópico \"temperatura\" de Kafka\n",
    "rawDF = spark.readStream\\\n",
    "              .format(\"kafka\")\\\n",
    "              .option(\"kafka.bootstrap.servers\", kafka_server)\\\n",
    "              .option(\"subscribe\", \"temperatura\")\\\n",
    "              .option(\"startingOffsets\", \"latest\")\\\n",
    "              .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919efb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer y transformar los datos del stream de Kafka\n",
    "# Convertir el valor del mensaje de Kafka a JSON y extraer los campos\n",
    "from pyspark.sql.functions import from_json\n",
    "valores = (rawDF\n",
    "           .selectExpr(\"CAST(value AS STRING) as json\")\n",
    "           .select(from_json(\"json\", \"sensor STRING, value INT\").alias(\"data\"))\n",
    "           .select(\"data.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el promedio de los valores cada 10 segundos usando una ventana de tiempo\n",
    "# Para esto, utilizamos la función `window` de PySpark\n",
    "# que permite \"ver\" los datos agrupados de cierta forma\n",
    "agregados = (valores\n",
    "             .groupBy(window(current_timestamp(), \"10 seconds\"))\n",
    "             .avg(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar la consulta de streaming para mostrar los resultados agregados en la consola\n",
    "query = (agregados.writeStream\n",
    "         .outputMode(\"complete\")\n",
    "         .format(\"console\")\n",
    "         .option(\"truncate\", False)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c762a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esperar a que la consulta de streaming termine (por ejemplo, durante 60 segundos)\n",
    "query.awaitTermination(60)\n",
    "query.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
