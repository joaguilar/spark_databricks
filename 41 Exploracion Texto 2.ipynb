{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97383041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spark-nlp==6.0.2 in ./.venv/lib/python3.9/site-packages (6.0.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (80.9.0)\n",
      "Requirement already satisfied: wordcloud in ./.venv/lib/python3.9/site-packages (1.9.4)\n",
      "Requirement already satisfied: pypdf2 in ./.venv/lib/python3.9/site-packages (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in ./.venv/lib/python3.9/site-packages (from pypdf2) (4.14.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (80.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spark-nlp==6.0.2 numpy pandas matplotlib seaborn setuptools wordcloud pypdf2\n",
    "%pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('./spark/spark-3.5.1-bin-hadoop3')\n",
    "from pyspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType, TimestampType, LongType\n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType, DecimalType\n",
    "from pyspark.sql.functions import regexp_extract, split, from_unixtime, col, avg, min, max\n",
    "from pyspark.sql.functions import grouping_id, window, explode, to_json, from_json\n",
    "from pyspark.sql.functions import udf, lit, current_timestamp, current_date, date_format\n",
    "import PyPDF2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd2c6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 15:30:15 WARN Utils: Your hostname, Joses-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 10.105.113.167 instead (on interface en0)\n",
      "25/07/27 15:30:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/joseaguilar/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/joseaguilar/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-97fd8a72-3830-4002-a4a7-39bbfaac5d74;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;6.0.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/joseaguilar/Documents/Development/spark/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in local-m2-cache\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in local-m2-cache\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in local-m2-cache\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in local-m2-cache\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in local-m2-cache\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in local-m2-cache\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.jsoup#jsoup;1.18.2 in central\n",
      "\tfound jakarta.mail#jakarta.mail-api;2.1.3 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;2.1.3 in central\n",
      "\tfound org.eclipse.angus#angus-mail;2.0.3 in central\n",
      "\tfound org.eclipse.angus#angus-activation;2.0.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml;4.1.2 in central\n",
      "\tfound org.apache.poi#poi;4.1.2 in central\n",
      "\tfound org.apache.commons#commons-collections4;4.4 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound com.zaxxer#SparseBitSet;1.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml-schemas;4.1.2 in central\n",
      "\tfound org.apache.xmlbeans#xmlbeans;3.1.0 in central\n",
      "\tfound org.apache.commons#commons-compress;1.19 in central\n",
      "\tfound com.github.virtuald#curvesapi;1.06 in central\n",
      "\tfound org.apache.poi#poi-scratchpad;4.1.2 in central\n",
      "\tfound org.apache.pdfbox#pdfbox;2.0.28 in central\n",
      "\tfound org.apache.pdfbox#fontbox;2.0.28 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.19.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.6 in central\n",
      "\tfound org.jetbrains#annotations;24.1.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 in central\n",
      ":: resolution report :: resolve 669ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.github.virtuald#curvesapi;1.06 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from local-m2-cache in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from local-m2-cache in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from local-m2-cache in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from local-m2-cache in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from local-m2-cache in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;6.0.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.19.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcom.zaxxer#SparseBitSet;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;2.1.3 from central in [default]\n",
      "\tjakarta.mail#jakarta.mail-api;2.1.3 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.commons#commons-collections4;4.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from local-m2-cache in [default]\n",
      "\torg.apache.pdfbox#fontbox;2.0.28 from central in [default]\n",
      "\torg.apache.pdfbox#pdfbox;2.0.28 from central in [default]\n",
      "\torg.apache.poi#poi;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml-schemas;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-scratchpad;4.1.2 from central in [default]\n",
      "\torg.apache.xmlbeans#xmlbeans;3.1.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-activation;2.0.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-mail;2.0.3 from central in [default]\n",
      "\torg.jetbrains#annotations;24.1.0 from central in [default]\n",
      "\torg.jsoup#jsoup;1.18.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\tcommons-codec#commons-codec;1.13 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  104  |   0   |   0   |   6   ||   98  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-97fd8a72-3830-4002-a4a7-39bbfaac5d74\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 98 already retrieved (0kB/8ms)\n",
      "25/07/27 15:30:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/27 15:30:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"kafkademo\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \",\".join([\n",
    "                \"com.johnsnowlabs.nlp:spark-nlp_2.12:6.0.2\"\n",
    "            ])) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397f601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0.2\n",
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 15:30:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, Normalizer\n",
    "\n",
    "print(sparknlp.version())\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ae0984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo: pdf/2303.10130v5.pdf\n",
      "Procesando archivo: pdf/1706.03762v7.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|       documento|               texto|\n",
      "+----------------+--------------------+\n",
      "|2303.10130v5.pdf|WORKING PAPER\\nGP...|\n",
      "+----------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textos_pdf = []\n",
    "for nombre_archivo in os.listdir(\"/Users/joseaguilar/Documents/Development/databricks/spark_databricks/pdf/\"):\n",
    "    if nombre_archivo.endswith(\".pdf\"):\n",
    "        ruta = os.path.join(\"pdf\", nombre_archivo)\n",
    "        print(f\"Procesando archivo: {ruta}\")\n",
    "        reader = PyPDF2.PdfReader(ruta)\n",
    "        # Extraer todo el texto concatenando páginas\n",
    "        texto = \"\"\n",
    "        for page in reader.pages:\n",
    "            texto += page.extract_text() + \" \"\n",
    "        textos_pdf.append((nombre_archivo, texto))\n",
    "\n",
    "# Crear un DataFrame de Spark con columnas 'documento' y 'texto'\n",
    "df_docs = spark.createDataFrame(textos_pdf, schema=[\"documento\", \"texto\"])\n",
    "df_docs.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da22100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------------------------------------------------------------------------------------+\n",
      "|       documento|                                                                                            palabras|\n",
      "+----------------+----------------------------------------------------------------------------------------------------+\n",
      "|2303.10130v5.pdf|[working, paper, gpts, are, gpts, an, early, look, at, the, labor, market, impact, potential, of,...|\n",
      "+----------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configurar una pipeline de Spark NLP para tokenizar\n",
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"linea_texto\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "# Opcional: normalizador para eliminar puntuación y convertir a minúsculas\n",
    "normalizer = Normalizer()\\\n",
    "    .setInputCols([\"token\"]).setOutputCol(\"token_norm\")\\\n",
    "    .setLowercase(True)\\\n",
    "    .setCleanupPatterns([\"[^\\\\w¿?¡!áéíóúÁÉÍÓÚüÜñÑ]\"])  # quitar símbolos no deseados\n",
    "\n",
    "finisher = Finisher()\\\n",
    "    .setInputCols([\"token_norm\"]).setOutputCols([\"palabras\"])\\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "# Reutilizar la pipeline de tokenización en la columna 'texto'\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "document_assembler.setInputCol(\"texto\")\n",
    "modelo_pdf = Pipeline(stages=[document_assembler, tokenizer, normalizer, finisher]).fit(df_docs)\n",
    "df_tokens_pdf = modelo_pdf.transform(df_docs).select(\"documento\", \"palabras\")\n",
    "df_tokens_pdf.show(1, truncate=100)  # ver tokens de un documento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cfdfc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|palabra|count|\n",
      "+-------+-----+\n",
      "|     de|  346|\n",
      "|      y|  129|\n",
      "|     la|  112|\n",
      "|     en|   91|\n",
      "|    del|   91|\n",
      "|  datos|   89|\n",
      "|   data|   86|\n",
      "|     el|   75|\n",
      "|    los|   75|\n",
      "|      a|   70|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode de la columna de listas de palabras para tener una palabra por fila\n",
    "from pyspark.sql.functions import explode, col\n",
    "df_palabras = df_tokens.select(explode(col(\"palabras\")).alias(\"palabra\"))\n",
    "# Filtrar posibles tokens vacíos o muy cortos si aplica\n",
    "df_palabras = df_palabras.filter(col(\"palabra\") != \"\")\n",
    "\n",
    "# Contar frecuencia de cada palabra\n",
    "df_freq = df_palabras.groupBy(\"palabra\").count().orderBy(col(\"count\").desc())\n",
    "df_freq.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "878def19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-----+----+\n",
      "|documento       |palabra|count|rank|\n",
      "+----------------+-------+-----+----+\n",
      "|1706.03762v7.pdf|the    |329  |1   |\n",
      "|1706.03762v7.pdf|of     |178  |2   |\n",
      "|1706.03762v7.pdf|and    |167  |3   |\n",
      "|1706.03762v7.pdf|in     |136  |4   |\n",
      "|1706.03762v7.pdf|to     |114  |5   |\n",
      "|2303.10130v5.pdf|the    |374  |1   |\n",
      "|2303.10130v5.pdf|and    |360  |2   |\n",
      "|2303.10130v5.pdf|of     |270  |3   |\n",
      "|2303.10130v5.pdf|to     |220  |4   |\n",
      "|2303.10130v5.pdf|in     |154  |5   |\n",
      "+----------------+-------+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Explotar las palabras por documento y contar frecuencias dentro de cada documento\n",
    "df_palabras_pdf = df_tokens_pdf.select(col(\"documento\"), explode(col(\"palabras\")).alias(\"palabra\"))\n",
    "df_freq_por_doc = df_palabras_pdf.groupBy(\"documento\", \"palabra\").count()\n",
    "# Por ejemplo, mostrar las 5 palabras más frecuentes de cada documento\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "window = Window.partitionBy(\"documento\").orderBy(F.desc(\"count\"))\n",
    "df_top5_por_doc = df_freq_por_doc.withColumn(\"rank\", F.row_number().over(window))\\\n",
    "                                 .filter(col(\"rank\") <= 5)\\\n",
    "                                 .orderBy(\"documento\", col(\"rank\"))\n",
    "df_top5_por_doc.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02cadb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       documento|count|\n",
      "+----------------+-----+\n",
      "|2303.10130v5.pdf|    2|\n",
      "|1706.03762v7.pdf|   97|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "term = \"attention\"  # término a buscar\n",
    "df_term_counts = df_palabras_pdf.filter(col(\"palabra\").rlike(term))\\\n",
    "                                .groupBy(\"documento\").count()\n",
    "df_term_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b7f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
